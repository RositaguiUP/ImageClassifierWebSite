{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial logistic regression\n",
    "\n",
    "Multinomial logistic regression is used to predict categorical placement in or the probability of category membership on a dependent variable based on multiple independent variables. The independent variables can be either dichotomous (i.e., binary) or continuous (i.e., interval or ratio in scale). Multinomial logistic regression is a simple extension of binary logistic regression that allows for more than two categories of the dependent or outcome variable. Like binary logistic regression, multinomial logistic regression uses maximum likelihood estimation to evaluate the probability of categorical membership.   \n",
    "\n",
    "Multinomial logistic regression does necessitate careful consideration of the sample size and examination for outlying cases. Like other data analysis procedures, initial data analysis should be thorough and include careful univariate, bivariate, and multivariate assessment. Specifically, multicollinearity should be evaluated with simple correlations among the independent variables. Also, multivariate diagnostics (i.e. standard multiple regression) can be used to assess for multivariate outliers and for the exclusion of outliers or influential cases. Sample size guidelines for multinomial logistic regression indicate a minimum of 10 cases per independent variable (Schwab, 2002).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import Output\n",
    "from plt_overfit import overfit_example, output\n",
    "from lab_utils_logistic import sigmoid\n",
    "# plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As any of the other regression algorithms we need a cost function, a gradient function and finally we will use it to do our approximation with the gradient descent algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function (logistic)\n",
    "_____\n",
    "\n",
    "We will reuse the same cost function that we developed for binary logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost (scalar):  cost \n",
    "    \"\"\"\n",
    "\n",
    "    m,n  = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b                                      #(n,)(n,)=scalar, see np.dot\n",
    "        f_wb_i = sigmoid(z_i)                                          #scalar\n",
    "        if f_wb_i == 0:\n",
    "          f_wb_i = 0.0000000000000001         \n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)      #scalar\n",
    "        \n",
    "    cost = cost/m                                                      #scalar\n",
    "\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j]**2)                                          #scalar\n",
    "    reg_cost = (lambda_/(2*m)) * reg_cost                              #scalar\n",
    "    \n",
    "    total_cost = cost + reg_cost                                       #scalar\n",
    "    return total_cost    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LetÂ´s test it just to make sure it is as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized cost: 0.6850849138741673\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,6)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1,)-0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient function (logistic)\n",
    "_____\n",
    "We will also reuse the gradient function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_): \n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    " \n",
    "    Args:\n",
    "      X (ndarray (m,n): Data, m examples with n features\n",
    "      y (ndarray (m,)): target values\n",
    "      w (ndarray (n,)): model parameters  \n",
    "      b (scalar)      : model parameter\n",
    "      lambda_ (scalar): Controls amount of regularization\n",
    "    Returns\n",
    "      dj_dw (ndarray Shape (n,)): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar)            : The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m,n = X.shape\n",
    "    dj_dw = np.zeros((n,))                            #(n,)\n",
    "    dj_db = 0.0                                       #scalar\n",
    "\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i],w) + b)          #(n,)(n,)=scalar\n",
    "        err_i  = f_wb_i  - y[i]                       #scalar\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i,j]      #scalar\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw/m                                   #(n,)\n",
    "    dj_db = dj_db/m                                   #scalar\n",
    "\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_/m) * w[j]\n",
    "\n",
    "    return dj_db, dj_dw  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dj_db: 0.341798994972791\n",
      "Regularized dj_dw:\n",
      " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5,3)\n",
    "y_tmp = np.array([0,1,0,1,0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent \n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, r_lambda, num_iters): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)   : Data, m examples with n features\n",
    "      y (ndarray (m,))   : target values\n",
    "      w_in (ndarray (n,)): Initial values of model parameters  \n",
    "      b_in (scalar)      : Initial values of model parameter\n",
    "      alpha (float)      : Learning rate\n",
    "      r_lambda (float)     : Regularization rate\n",
    "      num_iters (scalar) : number of iterations to run gradient descent\n",
    "      \n",
    "    Returns:\n",
    "      w (ndarray (n,))   : Updated values of parameters\n",
    "      b (scalar)         : Updated value of parameter \n",
    "    \"\"\"\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = compute_gradient_logistic_reg(X, y, w, b, r_lambda)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               \n",
    "        b = b - alpha * dj_db               \n",
    "      \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( compute_cost_logistic_reg(X, y, w, b, r_lambda) )\n",
    "        \n",
    "        pass\n",
    "        \n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "        \n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 0.8689039491005601   \n",
      "Iteration 1000: Cost 0.6794775943980855   \n",
      "Iteration 2000: Cost 0.6794775935781013   \n",
      "Iteration 3000: Cost 0.6794775935781012   \n",
      "Iteration 4000: Cost 0.6794775935781011   \n",
      "Iteration 5000: Cost 0.6794775935781011   \n",
      "Iteration 6000: Cost 0.6794775935781011   \n",
      "Iteration 7000: Cost 0.6794775935781011   \n",
      "Iteration 8000: Cost 0.6794775935781011   \n",
      "Iteration 9000: Cost 0.6794775935781011   \n",
      "\n",
      "updated parameters: w:[-0.21 -0.09], b:0.41147338701816333\n"
     ]
    }
   ],
   "source": [
    "x_train = np.random.rand(7,2)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1, 1])\n",
    "\n",
    "w_in = np.random.rand(x_train.shape[1])\n",
    "b_in = 0.5\n",
    "\n",
    "alph = 0.1\n",
    "r_lambda = 0.7\n",
    "iters = 10000\n",
    "\n",
    "w_out, b_out, _ = gradient_descent(x_train, y_train, w_in, b_in, alph, r_lambda, iters) \n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The changes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of Multiclass classification follows the same ideas as the binary classification. In multi-class classification, we have more than two classes. Here is an example. Say, we have different features and characteristics of cars, trucks, bikes, and boats as input features. Our job is to predict the label(car, truck, bike, or boat). We will treat each class as a binary classification problem; this approach is called the __one vs all method__.\n",
    "\n",
    "In the one vs all method, when we work with a class, that class is denoted by 1 and the rest of the classes becomes 0. For example, if we have four classes: cars, trucks, bikes, and boats. When we will work on the car, we will use the car as 1 and the rest of the classes as zeros. Again, when we will work on the truck, the element of the truck will be one, and the rest of the classes will be zeros\n",
    "\n",
    "| Car | Truck | Bike | Boat |\n",
    "| --- | --- | --- | --- |\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 0 | 0 | 0 | 1 |\n",
    "\n",
    "\n",
    "We will make one column for each of the classes with the same length as y. When the class is truck, we will make a column that has 1 for the rows with trucks and 0 otherwise.\n",
    "\n",
    "We will create a function that will produce a temporary y vector that will only have 1's for the selected class, and 0's for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y_change(y, cl):\n",
    "    \"\"\"\n",
    "    Creates an independent y vector that only holds 1's for\n",
    "    the selected class and zero for the rest\n",
    "    \n",
    "    Args:\n",
    "      y (ndarray (m,)) : target values\n",
    "      cl (scalar)      : The class we are studying.\n",
    "      \n",
    "    Returns:\n",
    "      y_pr (ndarray (n,))   : Array holding only 1's for the \n",
    "                              analyzed class.\n",
    "    \"\"\"\n",
    "    y_pr=[]\n",
    "    for i in range(0, len(y)):\n",
    "        if y[i] == cl:\n",
    "            y_pr.append(1)\n",
    "        else:\n",
    "            y_pr.append(0)\n",
    "    return y_pr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that will produce the w values that will separate the i-th class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_param(X, y):\n",
    "    \"\"\"\n",
    "    Creates the w_i vector for the given class.\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n)    : Data, m examples with n features\n",
    "      y (ndarray (m,))    : Target values\n",
    "      \n",
    "    Returns:\n",
    "      theta_list (ndarray (n,)) : This is a matrix that will hold a row for the w values\n",
    "                                  for every i class. \n",
    "    \"\"\"\n",
    "    alph = 0.1\n",
    "    r_lambda = 0.7\n",
    "    iters = 1000\n",
    "\n",
    "    y_uniq = list(set(y.flatten()))\n",
    "    theta_list = []\n",
    "    for i in y_uniq:\n",
    "        w_in = np.random.rand(X.shape[1])\n",
    "        b_in = 0.5\n",
    "\n",
    "        y_tr = pd.Series(y_change(y, i))\n",
    "        # y_tr = y_tr[:, np.newaxis]\n",
    "        np.array(y_tr)[:, np.newaxis]\n",
    "        print(f\"\\n\\nWe will find the weights for class: {i}\")\n",
    "        theta1, _ , _ = gradient_descent(X, y_tr, w_in, b_in, alph, r_lambda, iters) \n",
    "        theta_list.append(theta1)\n",
    "    return theta_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will create a function that should give us the proper class for every entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(theta_list, X, y):\n",
    "    y_uniq = list(set(y.flatten()))\n",
    "    y_hat = [0]*len(y)\n",
    "    for i in range(0, len(y_uniq)):\n",
    "        y_tr = y_change(y, y_uniq[i])\n",
    "        # y1 = sigmoid(x, theta_list[i])\n",
    "        y1 = sigmoid(np.dot(X, theta_list[i]))\n",
    "        for k in range(0, len(y)):\n",
    "            # if type(y1) == list:\n",
    "            if y_tr[k] == 1 and y1[k] >= 0.56:\n",
    "                y_hat[k] = y_uniq[i]\n",
    "            # else:\n",
    "                # if y_tr[k] == 1 and y1 >= 0.2:\n",
    "                    # y_hat[k] = y_uniq[i]\n",
    "    return y_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it. First, we will create the list of our weights for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "We will find the weights for class: 1\n",
      "Iteration    0: Cost 1.0676308054130494   \n",
      "Iteration  100: Cost 0.7641121223308954   \n",
      "Iteration  200: Cost 0.7609486835339567   \n",
      "Iteration  300: Cost 0.7580762211784994   \n",
      "Iteration  400: Cost 0.7553316992469886   \n",
      "Iteration  500: Cost 0.7527004652916653   \n",
      "Iteration  600: Cost 0.7501702532498109   \n",
      "Iteration  700: Cost 0.7477307117506597   \n",
      "Iteration  800: Cost 0.7453730907561653   \n",
      "Iteration  900: Cost 0.7430899785205556   \n",
      "\n",
      "\n",
      "We will find the weights for class: 2\n",
      "Iteration    0: Cost 0.9736917227044055   \n",
      "Iteration  100: Cost 0.7619995414389582   \n",
      "Iteration  200: Cost 0.7589694651211345   \n",
      "Iteration  300: Cost 0.7561846794248014   \n",
      "Iteration  400: Cost 0.7535223113780719   \n",
      "Iteration  500: Cost 0.7509685778528775   \n",
      "Iteration  600: Cost 0.7485118907260941   \n",
      "Iteration  700: Cost 0.7461424440202532   \n",
      "Iteration  800: Cost 0.7438519284438505   \n",
      "Iteration  900: Cost 0.7416332902177771   \n",
      "\n",
      "\n",
      "We will find the weights for class: 3\n",
      "Iteration    0: Cost 1.1363151864650998   \n",
      "Iteration  100: Cost 0.7617297724219352   \n",
      "Iteration  200: Cost 0.7585710838920531   \n",
      "Iteration  300: Cost 0.7557364244471381   \n",
      "Iteration  400: Cost 0.7530195784870781   \n",
      "Iteration  500: Cost 0.7504077165734949   \n",
      "Iteration  600: Cost 0.7478901267981881   \n",
      "Iteration  700: Cost 0.7454577442941153   \n",
      "Iteration  800: Cost 0.7431028804241591   \n",
      "Iteration  900: Cost 0.7408189965255737   \n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "f = open(\"features.csv\", \"r\")\n",
    "flag = True\n",
    "for line in f.readlines():\n",
    "    # x = ast.literal_eval(row[2])\n",
    "        if \",\" in line:\n",
    "            x = np.array(ast.literal_eval(line[line.index(\",\")+1:].replace(\"\\\"\", \"\")))\n",
    "            if flag:\n",
    "                normval = np.linalg.norm(ast.literal_eval(line[line.index(\",\")+1:].replace(\"\\\"\", \"\")))\n",
    "                flag = False\n",
    "            x_train.append(x/normval)\n",
    "            # x_train.append(ast.literal_eval(line[line.index(\",\")+1:].replace(\"\\\"\", \"\")))\n",
    "            aux = line[0:line.index(\",\")]\n",
    "            if aux == \"cat\":\n",
    "                y_train.append(1)\n",
    "            elif aux == \"duck\":\n",
    "                y_train.append(2)\n",
    "            elif aux == \"panda\":\n",
    "                y_train.append(3)    \n",
    "                \n",
    "x_train = np.array(x_train) \n",
    "y_train = np.array(y_train) \n",
    "                 \n",
    "theta_list = find_param(x_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have it, we can run our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "y_hat = predict(theta_list, x_train, y_train)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 [0, 0, 0]\n",
      "242 [0, 0, 0]\n",
      "243 [0, 0, 0]\n",
      "244 [0, 0, 0]\n",
      "245 [0, 0, 0]\n",
      "246 [0, 0, 0]\n",
      "247 [0, 0, 0]\n",
      "248 [0, 0, 0]\n",
      "249 [0, 0, 0]\n",
      "250 [0, 0, 0]\n",
      "251 [0, 0, 0]\n",
      "252 [0, 0, 0]\n",
      "253 [0, 0, 0]\n",
      "254 [0, 0, 0]\n",
      "255 [0, 0, 0]\n",
      "256 [0, 0, 0]\n",
      "257 [0, 0, 0]\n",
      "258 [0, 0, 0]\n",
      "259 [0, 0, 0]\n",
      "260 [0, 0, 0]\n",
      "261 [0, 0, 0]\n",
      "262 [0, 0, 0]\n",
      "263 [0, 0, 0]\n",
      "264 [0, 0, 0]\n",
      "265 [1, 0, 0]\n",
      "266 [0, 0, 0]\n",
      "267 [0, 0, 0]\n",
      "268 [0, 0, 0]\n",
      "269 [0, 0, 0]\n",
      "270 [0, 0, 0]\n",
      "271 [0, 0, 0]\n",
      "272 [0, 0, 0]\n",
      "273 [0, 0, 0]\n",
      "274 [1, 0, 0]\n",
      "275 [0, 0, 0]\n",
      "276 [0, 0, 0]\n",
      "277 [0, 0, 0]\n",
      "278 [0, 0, 0]\n",
      "279 [0, 0, 0]\n",
      "280 [1, 0, 0]\n",
      "281 [0, 0, 0]\n",
      "282 [0, 0, 0]\n",
      "283 [1, 0, 0]\n",
      "284 [0, 0, 0]\n",
      "285 [0, 0, 0]\n",
      "286 [0, 0, 0]\n",
      "287 [0, 0, 0]\n",
      "288 [0, 0, 0]\n",
      "289 [0, 0, 0]\n",
      "290 [0, 0, 0]\n",
      "291 [0, 0, 0]\n",
      "292 [0, 0, 0]\n",
      "293 [0, 0, 0]\n",
      "294 [0, 0, 0]\n",
      "295 [0, 0, 0]\n",
      "296 [0, 0, 0]\n",
      "297 [0, 0, 0]\n",
      "298 [0, 0, 0]\n",
      "299 [0, 0, 0]\n",
      "300 [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# PREDICTING VALUES\n",
    "from feature_extractor import *\n",
    "x_t = []\n",
    "for i in range(241, 301):\n",
    "    f = \"Images/tests/duck_{}.jpg\".format(i)\n",
    "    features = getFeatures(cv2.imread(f))\n",
    "    # y_train = [0, 1, 2]\n",
    "    x_t = np.array([features, features, features])/normval\n",
    "    # y_train = np.array(y_train)\n",
    "    y_hat = predict(theta_list, x_t, np.array([1, 2, 3]))\n",
    "    print(i, y_hat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now even observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (720,) and (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_42056/4069778103.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlinestyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Value'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Class'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Charlie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   3020\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mc:\\Users\\Charlie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \"\"\"\n\u001b[0;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Charlie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Charlie\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (720,) and (3,)"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bf17ac7f2c480e987d1e1c4a38a748",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plotting the actual and predicted values\n",
    "f1 = plt.figure()\n",
    "c = [i for i in range (1,len(y_train)+1,1)]\n",
    "plt.plot(c,y_train,color='r',linestyle='-')\n",
    "plt.plot(c,y_hat,color='b',linestyle='-')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Class')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the error\n",
    "f1 = plt.figure()\n",
    "c = [i for i in range(1,len(y_train)+1,1)]\n",
    "plt.plot(c,y_train-y_hat,color='green',linestyle='-')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Error Value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
